diff --git a/research/object_detection/dataset_tools/oid_hierarchical_labels_expansion.py b/research/object_detection/dataset_tools/oid_hierarchical_labels_expansion.py
index 013a0330..0320020b 100644
--- a/research/object_detection/dataset_tools/oid_hierarchical_labels_expansion.py
+++ b/research/object_detection/dataset_tools/oid_hierarchical_labels_expansion.py
@@ -35,7 +35,7 @@ def _update_dict(initial_dict, update):
    update: updated dictionary.
   """
 
-  for key, value_list in update.iteritems():
+  for key, value_list in update.items():
     if key in initial_dict:
       initial_dict[key].extend(value_list)
     else:
@@ -70,7 +70,7 @@ def _build_plain_hierarchy(hierarchy, skip_root=False):
   if not skip_root:
     all_keyed_parent[hierarchy['LabelName']] = all_children
     all_children = [hierarchy['LabelName']] + all_children
-    for child, _ in all_keyed_child.iteritems():
+    for child, _ in all_keyed_child.items():
       all_keyed_child[child].append(hierarchy['LabelName'])
     all_keyed_child[hierarchy['LabelName']] = []
 
diff --git a/research/object_detection/eval_util_test.py b/research/object_detection/eval_util_test.py
index d82efcbf..d3bc8fff 100644
--- a/research/object_detection/eval_util_test.py
+++ b/research/object_detection/eval_util_test.py
@@ -73,7 +73,7 @@ class EvalUtilTest(tf.test.TestCase):
 
     with self.test_session() as sess:
       metrics = {}
-      for key, (value_op, _) in metric_ops.iteritems():
+      for key, (value_op, _) in metric_ops.items():
         metrics[key] = value_op
       sess.run(update_op)
       metrics = sess.run(metrics)
@@ -93,7 +93,7 @@ class EvalUtilTest(tf.test.TestCase):
 
     with self.test_session() as sess:
       metrics = {}
-      for key, (value_op, _) in metric_ops.iteritems():
+      for key, (value_op, _) in metric_ops.items():
         metrics[key] = value_op
       sess.run(update_op_boxes)
       sess.run(update_op_masks)
@@ -113,7 +113,7 @@ class EvalUtilTest(tf.test.TestCase):
 
     with self.test_session() as sess:
       metrics = {}
-      for key, (value_op, _) in metric_ops.iteritems():
+      for key, (value_op, _) in metric_ops.items():
         metrics[key] = value_op
       sess.run(update_op_boxes)
       sess.run(update_op_masks)
diff --git a/research/object_detection/metrics/coco_evaluation.py b/research/object_detection/metrics/coco_evaluation.py
index e4276659..9f9bc7c7 100644
--- a/research/object_detection/metrics/coco_evaluation.py
+++ b/research/object_detection/metrics/coco_evaluation.py
@@ -536,7 +536,7 @@ class CocoMaskEvaluator(object_detection_evaluation.DetectionEvaluator):
         'annotations': self._groundtruth_list,
         'images': [{'id': image_id, 'height': shape[1], 'width': shape[2]}
                    for image_id, shape in self._image_id_to_mask_shape_map.
-                   iteritems()],
+                   items()],
         'categories': self._categories
     }
     coco_wrapped_groundtruth = coco_tools.COCOWrapper(
@@ -550,7 +550,7 @@ class CocoMaskEvaluator(object_detection_evaluation.DetectionEvaluator):
         include_metrics_per_category=self._include_metrics_per_category)
     mask_metrics.update(mask_per_category_ap)
     mask_metrics = {'DetectionMasks_'+ key: value
-                    for key, value in mask_metrics.iteritems()}
+                    for key, value in mask_metrics.items()}
     return mask_metrics
 
   def get_estimator_eval_metric_ops(self, image_id, groundtruth_boxes,
diff --git a/research/object_detection/metrics/coco_evaluation_test.py b/research/object_detection/metrics/coco_evaluation_test.py
index 47547e20..be2d6215 100644
--- a/research/object_detection/metrics/coco_evaluation_test.py
+++ b/research/object_detection/metrics/coco_evaluation_test.py
@@ -296,7 +296,7 @@ class CocoEvaluationPyFuncTest(tf.test.TestCase):
                    detection_classes: np.array([2])
                })
     metrics = {}
-    for key, (value_op, _) in eval_metric_ops.iteritems():
+    for key, (value_op, _) in eval_metric_ops.items():
       metrics[key] = value_op
     metrics = sess.run(metrics)
     self.assertAlmostEqual(metrics['DetectionBoxes_Precision/mAP'], 1.0)
@@ -392,7 +392,7 @@ class CocoEvaluationPyFuncTest(tf.test.TestCase):
                   np.array([2, 2])
           })
     metrics = {}
-    for key, (value_op, _) in eval_metric_ops.iteritems():
+    for key, (value_op, _) in eval_metric_ops.items():
       metrics[key] = value_op
     metrics = sess.run(metrics)
     self.assertAlmostEqual(metrics['DetectionBoxes_Precision/mAP'], 1.0)
@@ -450,7 +450,7 @@ class CocoEvaluationPyFuncTest(tf.test.TestCase):
                    detection_classes: np.array([[1], [3], [2]])
                })
     metrics = {}
-    for key, (value_op, _) in eval_metric_ops.iteritems():
+    for key, (value_op, _) in eval_metric_ops.items():
       metrics[key] = value_op
     metrics = sess.run(metrics)
     self.assertAlmostEqual(metrics['DetectionBoxes_Precision/mAP'], 1.0)
@@ -520,7 +520,7 @@ class CocoEvaluationPyFuncTest(tf.test.TestCase):
                    num_det_boxes_per_image: np.array([1, 1, 2]),
                })
     metrics = {}
-    for key, (value_op, _) in eval_metric_ops.iteritems():
+    for key, (value_op, _) in eval_metric_ops.items():
       metrics[key] = value_op
     metrics = sess.run(metrics)
     self.assertAlmostEqual(metrics['DetectionBoxes_Precision/mAP'], 1.0)
@@ -701,7 +701,7 @@ class CocoMaskEvaluationPyFuncTest(tf.test.TestCase):
                                            mode='constant')
                })
     metrics = {}
-    for key, (value_op, _) in eval_metric_ops.iteritems():
+    for key, (value_op, _) in eval_metric_ops.items():
       metrics[key] = value_op
     metrics = sess.run(metrics)
     self.assertAlmostEqual(metrics['DetectionMasks_Precision/mAP'], 1.0)
diff --git a/research/object_detection/metrics/oid_vrd_challenge_evaluation.py b/research/object_detection/metrics/oid_vrd_challenge_evaluation.py
index 20b93e66..410ab449 100644
--- a/research/object_detection/metrics/oid_vrd_challenge_evaluation.py
+++ b/research/object_detection/metrics/oid_vrd_challenge_evaluation.py
@@ -74,7 +74,7 @@ def _swap_labelmap_dict(labelmap_dict):
   Returns:
     A dictionary mapping class name to class numerical id.
   """
-  return dict((v, k) for k, v in labelmap_dict.iteritems())
+  return dict((v, k) for k, v in labelmap_dict.items())
 
 
 def main(parsed_args):
diff --git a/research/object_detection/model_lib.py b/research/object_detection/model_lib.py
index 23bcd0df..be7359e0 100644
--- a/research/object_detection/model_lib.py
+++ b/research/object_detection/model_lib.py
@@ -279,7 +279,7 @@ def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False):
     if mode in (tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL):
       losses_dict = detection_model.loss(
           prediction_dict, features[fields.InputDataFields.true_image_shape])
-      losses = [loss_tensor for loss_tensor in losses_dict.itervalues()]
+      losses = [loss_tensor for loss_tensor in losses_dict.values()]
       if train_config.add_regularization_loss:
         regularization_losses = tf.get_collection(
             tf.GraphKeys.REGULARIZATION_LOSSES)
@@ -378,7 +378,7 @@ def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False):
         eval_metrics = ['coco_detection_metrics']
       eval_metric_ops = eval_util.get_eval_metric_ops_for_evaluators(
           eval_metrics,
-          category_index.values(),
+          list(category_index.values()),
           eval_dict,
           include_metrics_per_category=eval_config.include_metrics_per_category)
       for loss_key, loss_tensor in iter(losses_dict.items()):
@@ -388,7 +388,7 @@ def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False):
       if img_summary is not None:
         eval_metric_ops['Detections_Left_Groundtruth_Right'] = (
             img_summary, tf.no_op())
-      eval_metric_ops = {str(k): v for k, v in eval_metric_ops.iteritems()}
+      eval_metric_ops = {str(k): v for k, v in eval_metric_ops.items()}
 
       if eval_config.use_moving_averages:
         variable_averages = tf.train.ExponentialMovingAverage(0.0)
diff --git a/research/object_detection/utils/object_detection_evaluation.py b/research/object_detection/utils/object_detection_evaluation.py
index 8a38d8c2..91426a2a 100644
--- a/research/object_detection/utils/object_detection_evaluation.py
+++ b/research/object_detection/utils/object_detection_evaluation.py
@@ -27,6 +27,8 @@ It supports the following operations:
 Note: This module operates on numpy boxes and box lists.
 """
 
+from __future__ import print_function
+
 from abc import ABCMeta
 from abc import abstractmethod
 import collections
@@ -839,9 +841,9 @@ class ObjectDetectionEvaluation(object):
       if self.use_weighted_mean_ap:
         all_scores = np.append(all_scores, scores)
         all_tp_fp_labels = np.append(all_tp_fp_labels, tp_fp_labels)
-      print 'Scores and tpfp per class label: {}'.format(class_index)
-      print tp_fp_labels
-      print scores
+      print('Scores and tpfp per class label: {}'.format(class_index))
+      print(tp_fp_labels)
+      print(scores)
       precision, recall = metrics.compute_precision_recall(
           scores, tp_fp_labels, self.num_gt_instances_per_class[class_index])
       self.precisions_per_class.append(precision)
diff --git a/research/object_detection/utils/vrd_evaluation.py b/research/object_detection/utils/vrd_evaluation.py
index f11f35ea..737e2ba7 100644
--- a/research/object_detection/utils/vrd_evaluation.py
+++ b/research/object_detection/utils/vrd_evaluation.py
@@ -252,12 +252,12 @@ class VRDDetectionEvaluator(object_detection_evaluation.DetectionEvaluator):
             recall_100,
     }
     if relationships:
-      for key, average_precision in average_precisions.iteritems():
+      for key, average_precision in average_precisions.items():
         vrd_metrics[self._metric_prefix + 'AP@{}IOU/{}'.format(
             self._matching_iou_threshold,
             relationships[key])] = average_precision
     else:
-      for key, average_precision in average_precisions.iteritems():
+      for key, average_precision in average_precisions.items():
         vrd_metrics[self._metric_prefix + 'AP@{}IOU/{}'.format(
             self._matching_iou_threshold, key)] = average_precision
 
@@ -547,7 +547,7 @@ class _VRDDetectionEvaluation(object):
       relation_field_values = np.concatenate(self._relation_field_values)
 
     for relation_field_value, _ in (
-        self._num_gt_instances_per_relationship.iteritems()):
+        self._num_gt_instances_per_relationship.items()):
       precisions, recalls = metrics.compute_precision_recall(
           scores[relation_field_values == relation_field_value],
           tp_fp_labels[relation_field_values == relation_field_value],
